{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # importing hugging face\n",
    "from nltk import sent_tokenize # sentence tokenizer, separate text to multiple sentences\n",
    "import nltk\n",
    "import torch\n",
    "from glob import glob # for file paths\n",
    "import pandas as pd # for tabular dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alvee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) LOAD THEME-CLASSIFIER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu->0 else 'cpu', we get : 0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large-mnli\"\n",
    "device = 0 if torch.cuda.is_available() else 'cpu' # if gpu present use gpu, 0 is gpu number 1\n",
    "# check if gpu or cpu. we get 0 so gpu\n",
    "print (f\"gpu->0 else 'cpu', we get : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE THEME-CLASSIFIER MODEL \n",
    "# but its not recommended to read model_name from global variable, this fucntion should be inside a class\n",
    "\n",
    "def load_model(device):\n",
    "    theme_classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=model_name,\n",
    "        device=device\n",
    "    )\n",
    "    return theme_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvee\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "theme_classifier = load_model(device) # install the model in puts it into gpu-mem?\n",
    "# should unload it as well after work done? search how to unload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_list = [\"friendship\", \"hope\", \"sacrifice\", \"battle\", \"self development\", \"betrayal\", \"love\", \"dialogue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I gave him a right hook then a left jab ',\n",
       " 'labels': ['battle',\n",
       "  'self development',\n",
       "  'sacrifice',\n",
       "  'dialogue',\n",
       "  'hope',\n",
       "  'love',\n",
       "  'betrayal',\n",
       "  'friendship'],\n",
       " 'scores': [0.47511664032936096,\n",
       "  0.16913151741027832,\n",
       "  0.10377558320760727,\n",
       "  0.10101887583732605,\n",
       "  0.0704294890165329,\n",
       "  0.030161136761307716,\n",
       "  0.029164331033825874,\n",
       "  0.021202413365244865]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to classify anything with the zero-shot classifier just call it\n",
    "# write any text and run it, it gives probability score for each class\n",
    "theme_classifier(\n",
    "    \"I gave him a right hook then a left jab \",\n",
    "    theme_list, \n",
    "    multi_labels=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/Subtitles\\\\Naruto Season 1 - 01.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 02.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 03.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 04.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 05.ass']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load subtitles dataset\n",
    "files = glob('../data/Subtitles/*.ass') # * means get all file with .ass \n",
    "files[:5] # test , print first 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A long time ago, a powerful demon fox appeared with nine tails.\\n',\n",
       " 'With its powerful tails,\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(files[0], 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    # data cleaning\n",
    "    lines = lines[27:] # data from line 27, because before it are meta data\n",
    "    lines = [\",\".join(line.split(',')[9:]) for line in lines ]# remove everything before the 9th comma , we just want the text\n",
    "    lines = [line.replace('\\\\N',' ') for line in lines ]# remove \\\\n from the text\n",
    "\n",
    "\n",
    "#  check \n",
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A long time ago, a powerful demon fox appeared with nine tails.\\n With its powerful tails,\\n it could smash mountains and create tidal waves.\\n A band of Ninjas rose to defend their village from attack.\\n We have to wait until the Fourth Hokage gets here!\\n We can't let it get any closer to our village!\\n One great Ninja was able to imprison the monster,\\n but died in the process.\\n This Ninja was known asâ€¦ the Fourth Hokage.\\n Naruto!\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine different sentences into one-big paragraph\n",
    "\" \".join(lines[:10]) # joins first 10 lines\n",
    "# this is the paragraph to be fed in theme-clf FNN, it can ake max 512 tokens input\n",
    "# so divide it in batches [:10] / [:20] etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract episode number from subitle file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all above from (2) Load Dataset in a single function : \n",
    "def load_subtitles_dataset(dataset_path):\n",
    "    subtitles_paths = glob(dataset_path + '/*.ass')\n",
    "    scripts = []\n",
    "    episode_num = []\n",
    "    for path in subtitles_paths: #path is the directory/file.ass\n",
    "        # print(f\"debug path: {path}\")\n",
    "        # from each path/ subtitle file read lines\n",
    "        with open(path, 'r', encoding='utf-8') as file: # without  encoding='utf-8' gives error, not shown in video\n",
    "            lines = file.readlines()\n",
    "            \n",
    "            # data/line cleaning\n",
    "            lines = lines[27:] # data from line 27, because before it are meta data\n",
    "            lines = [\",\".join(line.split(',')[9:]) for line in lines ]# remove everything before the 9th comma , we just want the text\n",
    "        \n",
    "        # data/line cleaning\n",
    "        lines = [line.replace('\\\\N',' ') for line in lines ]# remove \\\\n from the text\n",
    "        script = \" \".join(lines) # all texts to one big paragraph\n",
    "\n",
    "        episode = int(path.split('-')[-1].split('.')[0].strip())\n",
    "        scripts.append(script)\n",
    "        episode_num.append(episode)\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"episode\" : episode_num, \"script\": scripts})\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script\n",
       "0        1  A long time ago, a powerful demon fox appeared...\n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "2        3  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "3        4  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "4        5  C'mon!\\n Running like a fugitive,\\n Being chas..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"../data/Subtitles\" \n",
    "df = load_subtitles_dataset(dataset_path)\n",
    "\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.1) RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A long time ago, a powerful demon fox appeared with nine tails.',\n",
       " 'With its powerful tails,\\n it could smash mountains and create tidal waves.',\n",
       " 'A band of Ninjas rose to defend their village from attack.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load 'script' of the first episode [0]\n",
    "script = df.iloc[0]['script']\n",
    "\n",
    "# print script\n",
    "script # one long script\n",
    "\n",
    "# divide sciprt for tokenizer\n",
    "script_sentences = sent_tokenize(script)\n",
    "\n",
    "# print\n",
    "script_sentences[:3] # display first 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A long time ago, a powerful demon fox appeared with nine tails. With its powerful tails,\\n it could smash mountains and create tidal waves. A band of Ninjas rose to defend their village from attack. We have to wait until the Fourth Hokage gets here! We can't let it get any closer to our village! One great Ninja was able to imprison the monster,\\n but died in the process. This Ninja was known as… the Fourth Hokage. Naruto! Why did you do such a thing?! You're really gonna get it this time! I don't care! You know your problem? You can't do the things I do! Only I can do this! I'm better than all of you! Believe it! There's a problem, sir! Lord Hokage! What is it? Did that Naruto do something again?\",\n",
       " 'Yes. He climbed onto the Mountainside Images…\\n And he vandalized and graffitied all over them! Wait! Ha ha…\\n Why should I? Hey, Naruto! How did you suddenly get here, lruka Sensei? The question is what are you doing here when you should be in class now? Now listen, Naruto. You failed the last graduation test and the one before that. This is no time to be goofing off, you fool! We will have a re-test on the Transformation Jutsu! Even those who already passed will take it! Whaaaat?! Sakura Haruno. Here I go…\\n Transform! OK! I did it! Cha! Did you see that, Sasuke? Next, Sasuke Uchiha.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch sentences\n",
    "sentence_batch_size = 20\n",
    "script_batches = []\n",
    "for index in range(0, len(script_sentences), sentence_batch_size): # each indx jump by setence_batch_size which is 20\n",
    "    sent = \" \".join(script_sentences[index:index + sentence_batch_size])\n",
    "    script_batches.append(sent)\n",
    "\n",
    "# 20 sentences batched together \n",
    "script_batches[:2] # each 20 sentence is 1 batch, so [:2] display first 2 batch meaning 40 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put subtitle data into model : \n",
    "theme_output = theme_classifier(\n",
    "    script_batches[:2],\n",
    "    theme_list,\n",
    "    multi_label=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': [0.980073869228363, 0.9370123147964478],\n",
       " 'betrayal': [0.9396897554397583, 0.6457234621047974],\n",
       " 'battle': [0.854687511920929, 0.6581311225891113],\n",
       " 'sacrifice': [0.7349810004234314, 0.6258835792541504],\n",
       " 'self development': [0.7284947037696838, 0.8678191304206848],\n",
       " 'hope': [0.19909749925136566, 0.20424073934555054],\n",
       " 'friendship': [0.05922317877411842, 0.0860324501991272],\n",
       " 'love': [0.040261927992105484, 0.02802046574652195]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output \n",
    "theme_output\n",
    "\n",
    "# structured Output ==> battle :[batch_1_score,batch_2_score, ...] ==> battle : [0.3232,0.5645,..]\n",
    "themes = {}\n",
    "for output in theme_output:\n",
    "    for label,score in zip(output['labels'],output['scores']):\n",
    "        if label not in themes:\n",
    "            themes[label] = []\n",
    "        themes[label].append(score)\n",
    "\n",
    "# print strcutured theme outputs for each batches\n",
    "themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all above code from (2.1) run model in below function:\n",
    "def get_theme_inference(script):\n",
    "    script_sentences = sent_tokenize(script)\n",
    "    \n",
    "    # Batch sentences\n",
    "    # batch sentences\n",
    "    sentence_batch_size = 20\n",
    "    script_batches = []\n",
    "    for index in range(0, len(script_sentences), sentence_batch_size):\n",
    "        sent = \" \".join(script_sentences[index:index + sentence_batch_size])\n",
    "        script_batches.append(sent)\n",
    "\n",
    "    # Run model\n",
    "    theme_output = theme_classifier(\n",
    "        script_batches[:2],\n",
    "        theme_list,\n",
    "        multi_label=True\n",
    "    )\n",
    "    \n",
    "    # structured/wrangle Output\n",
    "    themes = {}\n",
    "    for output in theme_output:\n",
    "        for label,score in zip(output['labels'],output['scores']):\n",
    "            if label not in themes:\n",
    "                themes[label] = []\n",
    "            themes[label].append(score)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
