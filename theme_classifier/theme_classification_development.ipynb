{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # importing hugging face\n",
    "from nltk import sent_tokenize # sentence tokenizer, separate text to multiple sentences\n",
    "import nltk\n",
    "import torch\n",
    "from glob import glob # for file paths\n",
    "import pandas as pd # for tabular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alvee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) LOAD THEME-CLASSIFIER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu->0 else 'cpu', we get : 0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large-mnli\"\n",
    "device = 0 if torch.cuda.is_available() else 'cpu' # if gpu present use gpu, 0 is gpu number 1\n",
    "# check if gpu or cpu. we get 0 so gpu\n",
    "print (f\"gpu->0 else 'cpu', we get : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE THEME-CLASSIFIER MODEL \n",
    "# but its not recommended to read model_name from global variable, this fucntion should be inside a class\n",
    "\n",
    "def load_model(device):\n",
    "    theme_classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=model_name,\n",
    "        device=device\n",
    "    )\n",
    "    return theme_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvee\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "theme_classifier = load_model(device) # install the model in puts it into gpu-mem?\n",
    "# should unload it as well after work done? search how to unload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_list = [\"friendship\", \"hope\", \"sacrifice\", \"battle\", \"self development\", \"betrayal\", \"love\", \"dialogue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I gave him a right hook then a left jab ',\n",
       " 'labels': ['battle',\n",
       "  'self development',\n",
       "  'sacrifice',\n",
       "  'dialogue',\n",
       "  'hope',\n",
       "  'love',\n",
       "  'betrayal',\n",
       "  'friendship'],\n",
       " 'scores': [0.47511664032936096,\n",
       "  0.16913151741027832,\n",
       "  0.10377558320760727,\n",
       "  0.10101887583732605,\n",
       "  0.0704294890165329,\n",
       "  0.030161136761307716,\n",
       "  0.029164331033825874,\n",
       "  0.021202413365244865]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to classify anything with the zero-shot classifier just call it\n",
    "# write any text and run it, it gives probability score for each class\n",
    "theme_classifier(\n",
    "    \"I gave him a right hook then a left jab \",\n",
    "    theme_list, \n",
    "    multi_labels=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/Subtitles\\\\Naruto Season 1 - 01.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 02.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 03.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 04.ass',\n",
       " '../data/Subtitles\\\\Naruto Season 1 - 05.ass']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load subtitles dataset\n",
    "files = glob('../data/Subtitles/*.ass') # * means get all file with .ass \n",
    "files[:5] # test , print first 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A long time ago, a powerful demon fox appeared with nine tails.\\n',\n",
       " 'With its powerful tails,\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(files[0], 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    # data cleaning\n",
    "    lines = lines[27:] # data from line 27, because before it are meta data\n",
    "    lines = [\",\".join(line.split(',')[9:]) for line in lines ]# remove everything before the 9th comma , we just want the text\n",
    "    lines = [line.replace('\\\\N',' ') for line in lines ]# remove \\\\n from the text\n",
    "\n",
    "\n",
    "#  check \n",
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A long time ago, a powerful demon fox appeared with nine tails.\\n With its powerful tails,\\n it could smash mountains and create tidal waves.\\n A band of Ninjas rose to defend their village from attack.\\n We have to wait until the Fourth Hokage gets here!\\n We can't let it get any closer to our village!\\n One great Ninja was able to imprison the monster,\\n but died in the process.\\n This Ninja was known asâ€¦ the Fourth Hokage.\\n Naruto!\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine different sentences into one-big paragraph\n",
    "\" \".join(lines[:10]) # joins first 10 lines\n",
    "# this is the paragraph to be fed in theme-clf FNN, it can ake max 512 tokens input\n",
    "# so divide it in batches [:10] / [:20] etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract episode number from subitle file name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all above from (2) Load Dataset in a single function : \n",
    "def load_subtitles_dataset(dataset_path):\n",
    "    subtitles_paths = glob(dataset_path + '/*.ass')\n",
    "    scripts = []\n",
    "    episode_num = []\n",
    "    for path in subtitles_paths: #path is the directory/file.ass\n",
    "        # print(f\"debug path: {path}\")\n",
    "        # from each path/ subtitle file read lines\n",
    "        with open(path, 'r', encoding='utf-8') as file: # without  encoding='utf-8' gives error, not shown in video\n",
    "            lines = file.readlines()\n",
    "            \n",
    "            # data/line cleaning\n",
    "            lines = lines[27:] # data from line 27, because before it are meta data\n",
    "            lines = [\",\".join(line.split(',')[9:]) for line in lines ]# remove everything before the 9th comma , we just want the text\n",
    "        \n",
    "        # data/line cleaning\n",
    "        lines = [line.replace('\\\\N',' ') for line in lines ]# remove \\\\n from the text\n",
    "        script = \" \".join(lines) # all texts to one big paragraph\n",
    "\n",
    "        episode = int(path.split('-')[-1].split('.')[0].strip())\n",
    "        scripts.append(script)\n",
    "        episode_num.append(episode)\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"episode\" : episode_num, \"script\": scripts})\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script\n",
       "0        1  A long time ago, a powerful demon fox appeared...\n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "2        3  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "3        4  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "4        5  C'mon!\\n Running like a fugitive,\\n Being chas..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"../data/Subtitles\" \n",
    "df = load_subtitles_dataset(dataset_path)\n",
    "\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.1) RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A long time ago, a powerful demon fox appeared with nine tails.',\n",
       " 'With its powerful tails,\\n it could smash mountains and create tidal waves.',\n",
       " 'A band of Ninjas rose to defend their village from attack.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load 'script' of the first episode [0]\n",
    "script = df.iloc[0]['script']\n",
    "\n",
    "# print script\n",
    "script # one long script\n",
    "\n",
    "# divide sciprt for tokenizer\n",
    "script_sentences = sent_tokenize(script)\n",
    "\n",
    "# print\n",
    "script_sentences[:3] # display first 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No. Darn, this is going to be bad…\\n There\\'s no need to worry anymore. Lord Hokage! He\\'ll be back soon. Sensei, how much longer? OK, you may open your eyes now. Congratulations…on your graduation. In celebration, we\\'ll have ramen tonight! Iruka Sensei! That hurts! Naruto…\\n I was going to lecture to you...\\n that the road gets more difficult now that you\\'re a Ninja. But I guess I\\'ll just wait to tell you that until we get to the ramen stand…\\n W-What do you want, you little shrimp? Quit following me! You\\'re smaller than me and\\n you\\'re saying that you\\'re gonna become the Fifth Hokage? I don\\'t care if you are the 3rd Hokage\\'s grandson or not. It\\'s not that easy to be a Hokage! If you want it that bad, you\\'re gonna have to beat me first! Next episode:  \"My Name Is Konohamaru!\" Watch my outstanding performance!'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch sentences\n",
    "sentence_batch_size = 20\n",
    "script_batches = []\n",
    "for index in range(0, len(script_sentences), sentence_batch_size): # each indx jump by setence_batch_size which is 20\n",
    "    sent = \" \".join(script_sentences[index:index + sentence_batch_size])\n",
    "    script_batches.append(sent)\n",
    "\n",
    "# 20 sentences batched together \n",
    "script_batches[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
